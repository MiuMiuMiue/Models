{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_transformation(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_k, bias=False):\n",
    "        super().__init__()\n",
    "        # Transform the dimensionality to divisible by number of heads\n",
    "        assert(heads * d_k == d_model), \"d_model should be divisible by heads\"\n",
    "        self.linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x   : torch.Tensor\n",
    "            Input with shape (seq_len, batch_size, d_model)\n",
    "        \n",
    "        Return:\n",
    "        x   : torch.Tensor\n",
    "            Output with shape (seq_len, batch_size, heads, d_k)\n",
    "        \"\"\"\n",
    "\n",
    "        head_shape = x.shape[:-1]\n",
    "        x = self.linear(x)\n",
    "        # Split the x into heads number of matrices, same as reshape\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_headed_attention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        heads       : int\n",
    "                    Number of heads.\n",
    "        d_model     : int\n",
    "                    Number of features in the query, key, value vectors.\n",
    "        dropout_prob: float\n",
    "                    Drop out rate.\n",
    "        bias        : bool\n",
    "                    Whether we have the bias terms in linear transformation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calling the parent class constructor, inheriting from it\n",
    "        super(multi_headed_attention, self).__init__()\n",
    "\n",
    "        # number of features per head\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "\n",
    "        # Linear Transformation of the query, key, value matrices\n",
    "        self.query = linear_transformation(d_model, heads, self.d_k)\n",
    "        self.key = linear_transformation(d_model, heads, self.d_k)\n",
    "        self.value = linear_transformation(d_model, heads, self.d_k)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.scale = 1 / np.sqrt(self.d_k)\n",
    "        self.attn = None\n",
    "    \n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        # i = the word in query, j = the word in key.\n",
    "        # Iterate over all j's to get alignment score for the ith word in query over dimension d, which is self.d_k, for each head and batch.\n",
    "        # the result i, j corresponds to the alignment score of ith word in query and jth word in key.\n",
    "\n",
    "        # i, j = seq_len\n",
    "        # b = batch_size\n",
    "        # h = # of heads\n",
    "        # d = d_k\n",
    "        return torch.einsum(\"ibhd, jbhd->ijbh\", query, key)\n",
    "\n",
    "    def prepare_mask(self, mask: torch.Tensor, query_shape: List[int], key_shape: List[int]):\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "\n",
    "        mask = mask.unsqueeze(-1)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, *, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        seq_len, batch_size, _ = query.shape\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = self.prepare_mask(mask, query.shape, key.shape)\n",
    "\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)\n",
    "        value = self.value(value)\n",
    "\n",
    "        scores = self.get_scores(query, key)\n",
    "        scores *= self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attn = self.softmax(scores)\n",
    "        \n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # i, j = seq_len\n",
    "        # b = batch\n",
    "        # h = heads\n",
    "        # d = d_k\n",
    "\n",
    "        # for the wetght of jth word to i th word, we get the corresponding dth feature.\n",
    "        # append each dth feature of each word together according to the weights.\n",
    "        x = torch.einsum(\"ijbh, jbhd->ibhd\", attn, value)\n",
    "\n",
    "        x = x.view(seq_len, batch_size, -1) # concatenation of all heads\n",
    "\n",
    "        return self.output(x) # final linear transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p is the position and i is the dimension\n",
    "\n",
    "$\\begin{align}\n",
    "PE_{p,2i} &= sin\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg) \\\\\n",
    "PE_{p,2i + 1} &= cos\\Bigg(\\frac{p}{10000^{\\frac{2i}{d_{model}}}}\\Bigg)\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -------------------\n",
    "    d_model:    # of features of a word vector\n",
    "    max_len:    max length of a sentence\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    encodings = torch.zeros(max_len, d_model)\n",
    "    positions = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "    two_i = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
    "\n",
    "    div_term = torch.exp((-1 * two_i / d_model) * torch.log(10000))\n",
    "    encodings[:, 0::2] = torch.sin(positions * div_term)\n",
    "    encodings[:, 1::2] = torch.cos(positions * div_term)\n",
    "\n",
    "    encodings = encodings.unsqueeze(1).requires_grad(False) # input x shape should be (seq_len, batch, embed_size)\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_embedding_with_positional_encoding(nn.Module):\n",
    "    def __init__(self, d_model, n_vocab, max_len=5000):\n",
    "        super(word_embedding_with_positional_encoding, self).__init__()\n",
    "        self.embed = nn.Embedding(n_vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding(d_model, max_len))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pe = self.positional_encoding[:x.shape[0]].requires_grad(False)\n",
    "        embeddings = self.embed(x) * np.sqrt(self.d_model) + pe\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, bias, drop_out_rate):\n",
    "        super(feed_forward, self).__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_hidden, bias=bias)\n",
    "        self.layer2 = nn.Linear(d_hidden, d_model, bias=bias)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop_out = nn.Dropout(drop_out_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.layer1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.drop_out(y)\n",
    "        y = self.layer2(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_layer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, feed_forward, drop_out_rate, src_attn = None):\n",
    "        super(transformer_layer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.drop_out = nn.Dropout(drop_out_rate)\n",
    "        self.src_attn = src_attn\n",
    "        self.norm_self_attn = nn.LayerNorm([d_model])\n",
    "\n",
    "        if src_attn is not None:\n",
    "            self.norm_src_attn = nn.LayerNorm([d_model])\n",
    "        self.norm_feed_forward = nn.LayerNorm([d_model])\n",
    "    \n",
    "    def forward(self, x, mask, src = None, src_mask = None):\n",
    "        z = self.norm_self_attn(x)\n",
    "        self_attn = self.self_attn(query=z, key=z, value=z, mask=mask)\n",
    "        x = x + self.drop_out(self_attn)\n",
    "\n",
    "        if src is not None:\n",
    "            z = self.norm_src_attn(x)\n",
    "            src_attn = self.src_attn(query=z, key=src, value=src, mask=src_mask)\n",
    "            x = x + self.drop_out(src_attn)\n",
    "        \n",
    "        z = self.norm_feed_forward(x)\n",
    "        ff = self.feed_forward(z)\n",
    "\n",
    "        x = x + self.drop_out(ff)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_layers, self_attn, feed_forward, drop_out_rate):\n",
    "        super(encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([transformer_layer(d_model, self_attn, feed_forward, drop_out_rate) for _ in range(n_layers)])\n",
    "\n",
    "        self.norm = nn.LayerNorm([d_model])\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_layers, self_attn, feed_forward, drop_out_rate, src_attn):\n",
    "        super(decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([transformer_layer(d_model, self_attn, feed_forward, drop_out_rate, src_attn) for _ in n_layers])\n",
    "\n",
    "        self.norm = nn.LayerNorm([d_model])\n",
    "    \n",
    "    def forward(self, x, mask, src, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, src, src_mask)\n",
    "        \n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_prob, ff_hidden_size, n_layers, src_nvocab, tgt_nvocab):\n",
    "        encoder_attention = multi_headed_attention(heads=heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "        decoder_attention = multi_headed_attention(heads=heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "        encoder_decoder_attention = multi_headed_attention(heads=heads, d_model=d_model, dropout_prob=dropout_prob)\n",
    "\n",
    "        encoder_ffn = feed_forward(d_model=d_model, d_hidden=ff_hidden_size, bias=True, drop_out_rate=dropout_prob)\n",
    "        decoder_ffn = feed_forward(d_model=d_model, d_hidden=ff_hidden_size, bias=True, drop_out_rate=dropout_prob)\n",
    "\n",
    "        self.encoder = encoder(d_model=d_model, n_layers=n_layers, self_attn=encoder_attention, feed_forward=encoder_ffn, drop_out_rate=dropout_prob)\n",
    "        self.decoder = decoder(d_model=d_model, n_layers=n_layers, self_attn=decoder_attention, feed_forward=decoder_ffn, drop_out_rate=dropout_prob, src_attn=encoder_decoder_attention)\n",
    "\n",
    "        self.encode_embedding = word_embedding_with_positional_encoding(src_nvocab, d_model)\n",
    "        self.decode_embedding = word_embedding_with_positional_encoding(tgt_nvocab, d_model)\n",
    "\n",
    "        self.linear = nn.Linear(d_model, tgt_nvocab)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def src_embed(self, x):\n",
    "        return self.encode_embedding(x)\n",
    "    \n",
    "    def tgt_embed(self, x):\n",
    "        return self.decode_embedding(x)\n",
    "    \n",
    "    def generate_target_mask(self, tgt):\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        tri_mask = (1 - torch.triu(torch.ones(1, tgt.shape[0], tgt.shape[0]), diagonal=1)).bool()\n",
    "        return tgt_mask & tri_mask\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None):\n",
    "        encoded = self.encoder(x=src, mask=src_mask)\n",
    "        tgt_mask = self.generate_target_mask(tgt)\n",
    "        decoded = self.decoder(x=tgt, mask=tgt_mask, src=encoded, src_mask=src_mask)\n",
    "\n",
    "        result = self.linear(decoded)\n",
    "        result = self.softmax(result)\n",
    "\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
